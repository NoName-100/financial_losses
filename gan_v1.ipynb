{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our data look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.012495</td>\n",
       "      <td>0.011126</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.006625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.011439</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.006947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.007277</td>\n",
       "      <td>0.004049</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.017828</td>\n",
       "      <td>0.028210</td>\n",
       "      <td>0.007758</td>\n",
       "      <td>0.007382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.021115</td>\n",
       "      <td>0.019642</td>\n",
       "      <td>0.009238</td>\n",
       "      <td>0.011499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>741</td>\n",
       "      <td>0.001938</td>\n",
       "      <td>0.008833</td>\n",
       "      <td>0.003927</td>\n",
       "      <td>0.005106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>742</td>\n",
       "      <td>0.005003</td>\n",
       "      <td>0.018943</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.001988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>743</td>\n",
       "      <td>0.007683</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.007002</td>\n",
       "      <td>0.006467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>744</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.007621</td>\n",
       "      <td>0.001680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>745</td>\n",
       "      <td>0.004591</td>\n",
       "      <td>0.006675</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.009560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>746 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4\n",
       "0      0  0.012495  0.011126  0.003252  0.006625\n",
       "1      1  0.011439  0.002691  0.001206  0.006947\n",
       "2      2  0.000632  0.007277  0.004049  0.000074\n",
       "3      3  0.017828  0.028210  0.007758  0.007382\n",
       "4      4  0.021115  0.019642  0.009238  0.011499\n",
       "..   ...       ...       ...       ...       ...\n",
       "741  741  0.001938  0.008833  0.003927  0.005106\n",
       "742  742  0.005003  0.018943  0.003057  0.001988\n",
       "743  743  0.007683  0.001958  0.007002  0.006467\n",
       "744  744  0.003396  0.001280  0.007621  0.001680\n",
       "745  745  0.004591  0.006675  0.007600  0.009560\n",
       "\n",
       "[746 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(746, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(train_data.drop(labels=0, axis=1))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of GAN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You use GPU !\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('You use GPU !')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('You use CPU !')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative model\n",
    "def make_generator(noise_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_features=noise_dim, out_features=256),\n",
    "        nn.BatchNorm1d(256),\n",
    "        nn.LeakyReLU(),\n",
    "        \n",
    "        nn.Linear(in_features=256, out_features=64),\n",
    "        nn.BatchNorm1d(64),\n",
    "        nn.LeakyReLU(),\n",
    "        \n",
    "        nn.Linear(in_features=64, out_features=data_dim),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "\n",
    "# Discriminative model\n",
    "def make_discriminator():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_features=data_dim, out_features=256),\n",
    "        nn.BatchNorm1d(256),\n",
    "        nn.Sigmoid(),\n",
    "        \n",
    "        nn.Linear(in_features=256, out_features=64),\n",
    "        nn.BatchNorm1d(64),\n",
    "        nn.Sigmoid(),\n",
    "        \n",
    "        nn.Linear(in_features=64, out_features=2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 10\n",
    "generator = make_generator(noise_dim=noise_dim).to(device)\n",
    "discriminator = make_discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=256, bias=True)\n",
       "  (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): LeakyReLU(negative_slope=0.01)\n",
       "  (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): LeakyReLU(negative_slope=0.01)\n",
       "  (6): Linear(in_features=64, out_features=4, bias=True)\n",
       "  (7): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "  (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): Sigmoid()\n",
       "  (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): Sigmoid()\n",
       "  (6): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into training data and evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(np.array(train_data.drop(labels=0, axis=1)), dtype=torch.float)\n",
    "mu = X.mean(dim=0)\n",
    "theta = torch.sqrt(((X - mu)**2).mean(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x, mu, theta):\n",
    "    return (x - mu)/theta\n",
    "\n",
    "def denorm(x, mu, theta):\n",
    "    return x * theta + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = norm(X, mu, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rate = 0.2\n",
    "eval_index = int(X.shape[0] * (1 - eval_rate))\n",
    "X_train = X[0: eval_index]\n",
    "X_eval = X[eval_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "X_train_loader = torch.utils.data.DataLoader(X_train, batch_size, shuffle=True)\n",
    "X_eval_loader = torch.utils.data.DataLoader(X_eval, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose Adam Optimizer ($\\beta_1 = 0.9$, $\\beta_2 = 0.999$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "lr = 0.001\n",
    "gen_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "disc_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\tDisc. loss: 0.6615031003952027\tGen. loss: 0.784655487537384\n",
      "Epoch 2\tDisc. loss: 0.6444855034351349\tGen. loss: 0.802925193309784\n",
      "Epoch 3\tDisc. loss: 0.6654925525188446\tGen. loss: 0.752035665512085\n",
      "Epoch 4\tDisc. loss: 0.6570060312747955\tGen. loss: 0.7522339642047882\n",
      "Epoch 5\tDisc. loss: 0.6433203995227814\tGen. loss: 0.7690511226654053\n",
      "Epoch 6\tDisc. loss: 0.6386339664459229\tGen. loss: 0.8363366782665252\n",
      "Epoch 7\tDisc. loss: 0.6245865225791931\tGen. loss: 0.8306571424007416\n",
      "Epoch 8\tDisc. loss: 0.6037003517150878\tGen. loss: 0.9170199751853942\n",
      "Epoch 9\tDisc. loss: 0.5574246346950531\tGen. loss: 1.1058687150478363\n",
      "Epoch 10\tDisc. loss: 0.5001190006732941\tGen. loss: 1.3711613416671753\n",
      "Epoch 11\tDisc. loss: 0.43972674012184143\tGen. loss: 1.7944088697433471\n",
      "Epoch 12\tDisc. loss: 0.42942791879177095\tGen. loss: 2.2055188179016114\n",
      "Epoch 13\tDisc. loss: 0.35395767986774446\tGen. loss: 2.5256167888641357\n",
      "Epoch 14\tDisc. loss: 0.3154776573181152\tGen. loss: 2.7984646558761597\n",
      "Epoch 15\tDisc. loss: 0.26851602643728256\tGen. loss: 2.9775421619415283\n",
      "Epoch 16\tDisc. loss: 0.28153175860643387\tGen. loss: 3.2928536891937257\n",
      "Epoch 17\tDisc. loss: 0.24617708921432496\tGen. loss: 3.235081171989441\n",
      "Epoch 18\tDisc. loss: 0.1963487833738327\tGen. loss: 3.3755521535873414\n",
      "Epoch 19\tDisc. loss: 0.17261717841029167\tGen. loss: 3.3149561882019043\n",
      "Epoch 20\tDisc. loss: 0.16476022005081176\tGen. loss: 3.2763839244842528\n",
      "Epoch 21\tDisc. loss: 0.15402169972658158\tGen. loss: 3.331310248374939\n",
      "Epoch 22\tDisc. loss: 0.13218439146876335\tGen. loss: 3.2496416091918947\n",
      "Epoch 23\tDisc. loss: 0.13093433976173402\tGen. loss: 3.398430681228638\n",
      "Epoch 24\tDisc. loss: 0.11990457400679588\tGen. loss: 3.625261354446411\n",
      "Epoch 25\tDisc. loss: 0.10803158506751061\tGen. loss: 3.716060757637024\n",
      "Epoch 26\tDisc. loss: 0.09677732028067113\tGen. loss: 3.7250306606292725\n",
      "Epoch 27\tDisc. loss: 0.08661262057721615\tGen. loss: 3.5873838663101196\n",
      "Epoch 28\tDisc. loss: 0.08695350959897041\tGen. loss: 3.5690099000930786\n",
      "Epoch 29\tDisc. loss: 0.08884367421269417\tGen. loss: 3.6582913160324098\n",
      "Epoch 30\tDisc. loss: 0.10149182230234147\tGen. loss: 3.591555666923523\n",
      "Epoch 31\tDisc. loss: 0.08455785475671292\tGen. loss: 3.681939220428467\n",
      "Epoch 32\tDisc. loss: 0.08027230873703957\tGen. loss: 3.6101095914840697\n",
      "Epoch 33\tDisc. loss: 0.07370811626315117\tGen. loss: 3.534540224075317\n",
      "Epoch 34\tDisc. loss: 0.4059353083372116\tGen. loss: 3.735606145858765\n",
      "Epoch 35\tDisc. loss: 0.4187807649374008\tGen. loss: 2.780116629600525\n",
      "Epoch 36\tDisc. loss: 0.40931118428707125\tGen. loss: 2.0411956191062925\n",
      "Epoch 37\tDisc. loss: 0.38342698812484743\tGen. loss: 1.632342779636383\n",
      "Epoch 38\tDisc. loss: 0.34771101325750353\tGen. loss: 1.8431438088417054\n",
      "Epoch 39\tDisc. loss: 0.3401158809661865\tGen. loss: 2.32732834815979\n",
      "Epoch 40\tDisc. loss: 0.40604349672794343\tGen. loss: 2.395558500289917\n",
      "Epoch 41\tDisc. loss: 0.4307490199804306\tGen. loss: 2.2995585680007933\n",
      "Epoch 42\tDisc. loss: 0.42358538806438445\tGen. loss: 1.844121515750885\n",
      "Epoch 43\tDisc. loss: 0.4311927407979965\tGen. loss: 1.7836800336837768\n",
      "Epoch 44\tDisc. loss: 0.4178816735744476\tGen. loss: 1.5341882824897766\n",
      "Epoch 45\tDisc. loss: 0.3767495363950729\tGen. loss: 1.5479034423828124\n",
      "Epoch 46\tDisc. loss: 0.4015457481145859\tGen. loss: 1.5000234365463256\n",
      "Epoch 47\tDisc. loss: 0.36497150361537933\tGen. loss: 1.693677008152008\n",
      "Epoch 48\tDisc. loss: 0.3178299903869629\tGen. loss: 1.6495394349098205\n",
      "Epoch 49\tDisc. loss: 0.29768721759319305\tGen. loss: 1.8587557911872863\n",
      "Epoch 50\tDisc. loss: 0.2990686193108559\tGen. loss: 2.093909811973572\n",
      "Epoch 51\tDisc. loss: 0.3397964745759964\tGen. loss: 2.0044005513191223\n",
      "Epoch 52\tDisc. loss: 0.3655818045139313\tGen. loss: 2.238218057155609\n",
      "Epoch 53\tDisc. loss: 0.354003033041954\tGen. loss: 1.8631451964378356\n",
      "Epoch 54\tDisc. loss: 0.33888696432113646\tGen. loss: 1.7469628930091858\n",
      "Epoch 55\tDisc. loss: 0.34836560040712355\tGen. loss: 1.8494929194450378\n",
      "Epoch 56\tDisc. loss: 0.3366262674331665\tGen. loss: 2.044238579273224\n",
      "Epoch 57\tDisc. loss: 0.3082511141896248\tGen. loss: 1.9913381695747376\n",
      "Epoch 58\tDisc. loss: 0.2714254051446915\tGen. loss: 2.1495365738868712\n",
      "Epoch 59\tDisc. loss: 0.32338899523019793\tGen. loss: 2.6099591493606566\n",
      "Epoch 60\tDisc. loss: 0.2719300091266632\tGen. loss: 2.5357454776763917\n",
      "Epoch 61\tDisc. loss: 0.2635371327400208\tGen. loss: 2.4886865615844727\n",
      "Epoch 62\tDisc. loss: 0.26914351880550386\tGen. loss: 2.3466594219207764\n",
      "Epoch 63\tDisc. loss: 0.2957689374685287\tGen. loss: 2.0860638499259947\n",
      "Epoch 64\tDisc. loss: 0.3527109012007713\tGen. loss: 2.074710261821747\n",
      "Epoch 65\tDisc. loss: 0.2871826320886612\tGen. loss: 2.161745071411133\n",
      "Epoch 66\tDisc. loss: 0.25282030254602433\tGen. loss: 2.1989988803863527\n",
      "Epoch 67\tDisc. loss: 0.24976890087127684\tGen. loss: 2.218079376220703\n",
      "Epoch 68\tDisc. loss: 0.24341695755720139\tGen. loss: 2.499028182029724\n",
      "Epoch 69\tDisc. loss: 0.21402586698532106\tGen. loss: 2.8147937536239622\n",
      "Epoch 70\tDisc. loss: 0.18067748695611954\tGen. loss: 2.9389931678771974\n",
      "Epoch 71\tDisc. loss: 0.18455928117036818\tGen. loss: 3.3582202672958372\n",
      "Epoch 72\tDisc. loss: 0.15368107482790946\tGen. loss: 3.607453489303589\n",
      "Epoch 73\tDisc. loss: 0.12845406793057917\tGen. loss: 3.2498165130615235\n",
      "Epoch 74\tDisc. loss: 0.11208193562924862\tGen. loss: 3.4912102699279783\n",
      "Epoch 75\tDisc. loss: 0.10986329466104508\tGen. loss: 3.442095732688904\n",
      "Epoch 76\tDisc. loss: 0.11504776328802109\tGen. loss: 3.5753185987472533\n",
      "Epoch 77\tDisc. loss: 0.165584297478199\tGen. loss: 3.316891384124756\n",
      "Epoch 78\tDisc. loss: 0.2641172051429749\tGen. loss: 2.77698974609375\n",
      "Epoch 79\tDisc. loss: 0.2571060582995415\tGen. loss: 2.80225555896759\n",
      "Epoch 80\tDisc. loss: 0.2668179377913475\tGen. loss: 2.8481090307235717\n",
      "Epoch 81\tDisc. loss: 0.2878831043839455\tGen. loss: 2.6550005435943604\n",
      "Epoch 82\tDisc. loss: 0.30475457310676574\tGen. loss: 2.4633083820343016\n",
      "Epoch 83\tDisc. loss: 0.2852941855788231\tGen. loss: 2.463400387763977\n",
      "Epoch 84\tDisc. loss: 0.30465125739574433\tGen. loss: 2.511795234680176\n",
      "Epoch 85\tDisc. loss: 0.2664023250341415\tGen. loss: 2.426242399215698\n",
      "Epoch 86\tDisc. loss: 0.24027833044528962\tGen. loss: 2.4796454668045045\n",
      "Epoch 87\tDisc. loss: 0.24649715274572373\tGen. loss: 2.447513151168823\n",
      "Epoch 88\tDisc. loss: 0.25239416360855105\tGen. loss: 2.469559931755066\n",
      "Epoch 89\tDisc. loss: 0.24468810260295867\tGen. loss: 2.370068073272705\n",
      "Epoch 90\tDisc. loss: 0.23849737048149108\tGen. loss: 2.5064210653305055\n",
      "Epoch 91\tDisc. loss: 0.22862858176231385\tGen. loss: 2.6625088453292847\n",
      "Epoch 92\tDisc. loss: 0.2428329259157181\tGen. loss: 2.5652183294296265\n",
      "Epoch 93\tDisc. loss: 0.2547812953591347\tGen. loss: 2.675782632827759\n",
      "Epoch 94\tDisc. loss: 0.20901037752628326\tGen. loss: 2.682970380783081\n",
      "Epoch 95\tDisc. loss: 0.2148650050163269\tGen. loss: 2.7405012369155886\n",
      "Epoch 96\tDisc. loss: 0.1846481278538704\tGen. loss: 2.6441097497940063\n",
      "Epoch 97\tDisc. loss: 0.17468321099877357\tGen. loss: 2.6419985055923463\n",
      "Epoch 98\tDisc. loss: 0.17568152397871017\tGen. loss: 2.5507456779479982\n",
      "Epoch 99\tDisc. loss: 0.19378843903541565\tGen. loss: 2.7070285797119142\n",
      "Epoch 100\tDisc. loss: 0.185561840236187\tGen. loss: 2.623446297645569\n",
      "Epoch 101\tDisc. loss: 0.18886388689279557\tGen. loss: 2.729288840293884\n",
      "Epoch 102\tDisc. loss: 0.1813353978097439\tGen. loss: 2.8955075263977053\n",
      "Epoch 103\tDisc. loss: 0.16044294387102126\tGen. loss: 2.781908130645752\n",
      "Epoch 104\tDisc. loss: 0.179656433314085\tGen. loss: 2.92646005153656\n",
      "Epoch 105\tDisc. loss: 0.1645168997347355\tGen. loss: 2.7530747652053833\n",
      "Epoch 106\tDisc. loss: 0.1676538974046707\tGen. loss: 2.7531091451644896\n",
      "Epoch 107\tDisc. loss: 0.17335807159543037\tGen. loss: 2.766686749458313\n",
      "Epoch 108\tDisc. loss: 0.1906332328915596\tGen. loss: 2.949613094329834\n",
      "Epoch 109\tDisc. loss: 0.1830092377960682\tGen. loss: 3.0448473930358886\n",
      "Epoch 110\tDisc. loss: 0.19304929822683334\tGen. loss: 2.995973515510559\n",
      "Epoch 111\tDisc. loss: 0.2085434839129448\tGen. loss: 2.7765098810195923\n",
      "Epoch 112\tDisc. loss: 0.4490639865398407\tGen. loss: 1.8174989581108094\n",
      "Epoch 113\tDisc. loss: 0.4403874486684799\tGen. loss: 1.8474952697753906\n",
      "Epoch 114\tDisc. loss: 0.46252407431602477\tGen. loss: 1.7441294074058533\n",
      "Epoch 115\tDisc. loss: 0.4554224967956543\tGen. loss: 1.9220006346702576\n",
      "Epoch 116\tDisc. loss: 0.3944955408573151\tGen. loss: 1.3111884117126464\n",
      "Epoch 117\tDisc. loss: 0.39322909712791443\tGen. loss: 1.215738046169281\n",
      "Epoch 118\tDisc. loss: 0.3738402783870697\tGen. loss: 1.2081750512123108\n",
      "Epoch 119\tDisc. loss: 0.3605510801076889\tGen. loss: 1.3796290040016175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120\tDisc. loss: 0.3715049773454666\tGen. loss: 1.3776726365089416\n",
      "Epoch 121\tDisc. loss: 0.3663812935352325\tGen. loss: 1.3831565260887146\n",
      "Epoch 122\tDisc. loss: 0.38091476261615753\tGen. loss: 1.3772352814674378\n",
      "Epoch 123\tDisc. loss: 0.36439929008483884\tGen. loss: 1.5318590879440308\n",
      "Epoch 124\tDisc. loss: 0.3757751166820526\tGen. loss: 1.427037286758423\n",
      "Epoch 125\tDisc. loss: 0.39669119417667387\tGen. loss: 1.56555677652359\n",
      "Epoch 126\tDisc. loss: 0.369711372256279\tGen. loss: 1.5496949076652526\n",
      "Epoch 127\tDisc. loss: 0.35482153594493865\tGen. loss: 1.5303675651550293\n",
      "Epoch 128\tDisc. loss: 0.37688611447811127\tGen. loss: 1.6037490487098693\n",
      "Epoch 129\tDisc. loss: 0.37465394139289854\tGen. loss: 1.6110775232315064\n",
      "Epoch 130\tDisc. loss: 0.3512293934822083\tGen. loss: 1.7059202075004578\n",
      "Epoch 131\tDisc. loss: 0.34711042046546936\tGen. loss: 1.4977964282035827\n",
      "Epoch 132\tDisc. loss: 0.3535922408103943\tGen. loss: 1.6490043640136718\n",
      "Epoch 133\tDisc. loss: 0.34471082091331484\tGen. loss: 1.6365487098693847\n",
      "Epoch 134\tDisc. loss: 0.3365966796875\tGen. loss: 1.5593045949935913\n",
      "Epoch 135\tDisc. loss: 0.3501644104719162\tGen. loss: 1.495245373249054\n",
      "Epoch 136\tDisc. loss: 0.31675143986940385\tGen. loss: 1.6262908816337585\n",
      "Epoch 137\tDisc. loss: 0.34974640011787417\tGen. loss: 1.3738197326660155\n",
      "Epoch 138\tDisc. loss: 0.3346058793365955\tGen. loss: 1.5105186581611634\n",
      "Epoch 139\tDisc. loss: 0.36272862255573274\tGen. loss: 1.424495482444763\n",
      "Epoch 140\tDisc. loss: 0.33517205268144606\tGen. loss: 1.4940675377845765\n",
      "Epoch 141\tDisc. loss: 0.3273409575223923\tGen. loss: 1.6384915709495544\n",
      "Epoch 142\tDisc. loss: 0.34584251046180725\tGen. loss: 1.6291349172592162\n",
      "Epoch 143\tDisc. loss: 0.33424134105443953\tGen. loss: 1.6976264357566833\n",
      "Epoch 144\tDisc. loss: 0.35046633183956144\tGen. loss: 1.4431545734405518\n",
      "Epoch 145\tDisc. loss: 0.3258413761854172\tGen. loss: 1.592405879497528\n",
      "Epoch 146\tDisc. loss: 0.3456458508968353\tGen. loss: 1.6932560443878173\n",
      "Epoch 147\tDisc. loss: 0.3199365109205246\tGen. loss: 1.61730877161026\n",
      "Epoch 148\tDisc. loss: 0.3512911409139633\tGen. loss: 1.877340841293335\n",
      "Epoch 149\tDisc. loss: 0.3449268937110901\tGen. loss: 1.7447408080101012\n",
      "Epoch 150\tDisc. loss: 0.3529332220554352\tGen. loss: 1.6248609185218812\n",
      "Epoch 151\tDisc. loss: 0.36633881330490115\tGen. loss: 1.9070513129234314\n",
      "Epoch 152\tDisc. loss: 0.33925868272781373\tGen. loss: 1.6376500844955444\n",
      "Epoch 153\tDisc. loss: 0.35143159329891205\tGen. loss: 1.6914281129837037\n",
      "Epoch 154\tDisc. loss: 0.3242634743452072\tGen. loss: 1.5146127104759217\n",
      "Epoch 155\tDisc. loss: 0.3293197304010391\tGen. loss: 1.7224264979362487\n",
      "Epoch 156\tDisc. loss: 0.3627812176942825\tGen. loss: 1.888886308670044\n",
      "Epoch 157\tDisc. loss: 0.33765536546707153\tGen. loss: 1.4928874850273133\n",
      "Epoch 158\tDisc. loss: 0.3460096329450607\tGen. loss: 1.9427332282066345\n",
      "Epoch 159\tDisc. loss: 0.3237771049141884\tGen. loss: 1.5333751678466796\n",
      "Epoch 160\tDisc. loss: 0.33226041346788404\tGen. loss: 1.6034423232078552\n",
      "Epoch 161\tDisc. loss: 0.3204949349164963\tGen. loss: 1.717109704017639\n",
      "Epoch 162\tDisc. loss: 0.32184077203273775\tGen. loss: 1.6940136075019836\n",
      "Epoch 163\tDisc. loss: 0.32737341821193694\tGen. loss: 1.6143977284431457\n",
      "Epoch 164\tDisc. loss: 0.3109655648469925\tGen. loss: 1.6359203934669495\n",
      "Epoch 165\tDisc. loss: 0.3237981528043747\tGen. loss: 1.7615407705307007\n",
      "Epoch 166\tDisc. loss: 0.32394462525844575\tGen. loss: 1.7852462887763978\n",
      "Epoch 167\tDisc. loss: 0.31741593927145006\tGen. loss: 1.7163211822509765\n",
      "Epoch 168\tDisc. loss: 0.3161208838224411\tGen. loss: 1.6049646973609923\n",
      "Epoch 169\tDisc. loss: 0.3244790256023407\tGen. loss: 1.6975112199783324\n",
      "Epoch 170\tDisc. loss: 0.32521732449531554\tGen. loss: 1.5559140205383302\n",
      "Epoch 171\tDisc. loss: 0.3197962373495102\tGen. loss: 1.6753705143928528\n",
      "Epoch 172\tDisc. loss: 0.3251856803894043\tGen. loss: 1.5963056802749633\n",
      "Epoch 173\tDisc. loss: 0.319096539914608\tGen. loss: 1.5493039965629578\n",
      "Epoch 174\tDisc. loss: 0.31870380192995074\tGen. loss: 1.680996036529541\n",
      "Epoch 175\tDisc. loss: 0.31148643493652345\tGen. loss: 1.7206098198890687\n",
      "Epoch 176\tDisc. loss: 0.33959809243679046\tGen. loss: 2.121246361732483\n",
      "Epoch 177\tDisc. loss: 0.3324117913842201\tGen. loss: 1.5895897984504699\n",
      "Epoch 178\tDisc. loss: 0.3110079124569893\tGen. loss: 1.7231818914413453\n",
      "Epoch 179\tDisc. loss: 0.323715141415596\tGen. loss: 1.7595141887664796\n",
      "Epoch 180\tDisc. loss: 0.33202888667583463\tGen. loss: 1.6585354208946228\n",
      "Epoch 181\tDisc. loss: 0.32907616198062895\tGen. loss: 1.7656318187713622\n",
      "Epoch 182\tDisc. loss: 0.3340562045574188\tGen. loss: 1.745173180103302\n",
      "Epoch 183\tDisc. loss: 0.3346736192703247\tGen. loss: 1.5399736642837525\n",
      "Epoch 184\tDisc. loss: 0.3333071082830429\tGen. loss: 1.673298144340515\n",
      "Epoch 185\tDisc. loss: 0.3362003579735756\tGen. loss: 1.6000436425209046\n",
      "Epoch 186\tDisc. loss: 0.363718843460083\tGen. loss: 1.7367494463920594\n",
      "Epoch 187\tDisc. loss: 0.3357601881027222\tGen. loss: 1.5536637544631957\n",
      "Epoch 188\tDisc. loss: 0.3299182951450348\tGen. loss: 1.6865986466407776\n",
      "Epoch 189\tDisc. loss: 0.3250391334295273\tGen. loss: 1.8712764501571655\n",
      "Epoch 190\tDisc. loss: 0.31293614506721495\tGen. loss: 1.6019659042358398\n",
      "Epoch 191\tDisc. loss: 0.3057172894477844\tGen. loss: 1.7097578525543213\n",
      "Epoch 192\tDisc. loss: 0.3058606430888176\tGen. loss: 1.7106348037719727\n",
      "Epoch 193\tDisc. loss: 0.31788126975297926\tGen. loss: 1.846796488761902\n",
      "Epoch 194\tDisc. loss: 0.33249332010746\tGen. loss: 1.6974150896072389\n",
      "Epoch 195\tDisc. loss: 0.32118166536092757\tGen. loss: 1.6207024097442626\n",
      "Epoch 196\tDisc. loss: 0.30172460377216337\tGen. loss: 1.745316982269287\n",
      "Epoch 197\tDisc. loss: 0.30123984813690186\tGen. loss: 1.6171772360801697\n",
      "Epoch 198\tDisc. loss: 0.3211172461509705\tGen. loss: 1.6041321635246277\n",
      "Epoch 199\tDisc. loss: 0.3255524307489395\tGen. loss: 1.6254592537879944\n",
      "Epoch 200\tDisc. loss: 0.30754335075616834\tGen. loss: 1.6610915064811707\n",
      "Epoch 201\tDisc. loss: 0.3212389975786209\tGen. loss: 2.0349276185035707\n",
      "Epoch 202\tDisc. loss: 0.312839487195015\tGen. loss: 1.6702199935913087\n",
      "Epoch 203\tDisc. loss: 0.3009367510676384\tGen. loss: 2.0216379880905153\n",
      "Epoch 204\tDisc. loss: 0.2954681873321533\tGen. loss: 1.8799310326576233\n",
      "Epoch 205\tDisc. loss: 0.31860648691654203\tGen. loss: 1.8827738523483277\n",
      "Epoch 206\tDisc. loss: 0.3205175340175629\tGen. loss: 1.7633384108543395\n",
      "Epoch 207\tDisc. loss: 0.3132929354906082\tGen. loss: 1.8206138372421266\n",
      "Epoch 208\tDisc. loss: 0.30736691802740096\tGen. loss: 1.7195182919502259\n",
      "Epoch 209\tDisc. loss: 0.3101069748401642\tGen. loss: 1.9216910600662231\n",
      "Epoch 210\tDisc. loss: 0.31617788672447206\tGen. loss: 1.7648443579673767\n",
      "Epoch 211\tDisc. loss: 0.3208704352378845\tGen. loss: 1.6935843586921693\n",
      "Epoch 212\tDisc. loss: 0.3119456350803375\tGen. loss: 1.560822570323944\n",
      "Epoch 213\tDisc. loss: 0.32059362828731536\tGen. loss: 1.6936039328575134\n",
      "Epoch 214\tDisc. loss: 0.29892024099826814\tGen. loss: 1.7299196243286132\n",
      "Epoch 215\tDisc. loss: 0.2947687730193138\tGen. loss: 1.6615615129470824\n",
      "Epoch 216\tDisc. loss: 0.2957754909992218\tGen. loss: 1.83685702085495\n",
      "Epoch 217\tDisc. loss: 0.2986807480454445\tGen. loss: 1.7393238544464111\n",
      "Epoch 218\tDisc. loss: 0.3060297638177872\tGen. loss: 1.8655773401260376\n",
      "Epoch 219\tDisc. loss: 0.29168234169483187\tGen. loss: 1.5783056974411012\n",
      "Epoch 220\tDisc. loss: 0.31009499430656434\tGen. loss: 1.80941162109375\n",
      "Epoch 221\tDisc. loss: 0.31682718843221663\tGen. loss: 1.512355124950409\n",
      "Epoch 222\tDisc. loss: 0.3179231733083725\tGen. loss: 1.7695254802703857\n",
      "Epoch 223\tDisc. loss: 0.30564433336257935\tGen. loss: 1.5503161668777465\n",
      "Epoch 224\tDisc. loss: 0.314566445350647\tGen. loss: 1.8223632335662843\n",
      "Epoch 225\tDisc. loss: 0.3559347987174988\tGen. loss: 1.867267906665802\n",
      "Epoch 226\tDisc. loss: 0.3251383453607559\tGen. loss: 1.7889363408088683\n",
      "Epoch 227\tDisc. loss: 0.3370541676878929\tGen. loss: 1.9919044733047486\n",
      "Epoch 228\tDisc. loss: 0.32840867191553114\tGen. loss: 2.0108797907829286\n",
      "Epoch 229\tDisc. loss: 0.30816368460655214\tGen. loss: 1.9174074053764343\n",
      "Epoch 230\tDisc. loss: 0.29642118513584137\tGen. loss: 2.0371615171432493\n",
      "Epoch 231\tDisc. loss: 0.29559551179409027\tGen. loss: 1.846347737312317\n",
      "Epoch 232\tDisc. loss: 0.2978640228509903\tGen. loss: 1.7164959549903869\n",
      "Epoch 233\tDisc. loss: 0.3265921026468277\tGen. loss: 1.9558523416519165\n",
      "Epoch 234\tDisc. loss: 0.32933689206838607\tGen. loss: 1.7262014746665955\n",
      "Epoch 235\tDisc. loss: 0.33608231544494627\tGen. loss: 1.803961491584778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236\tDisc. loss: 0.3401243597269058\tGen. loss: 1.6558172583580018\n",
      "Epoch 237\tDisc. loss: 0.3138796463608742\tGen. loss: 1.5823472380638122\n",
      "Epoch 238\tDisc. loss: 0.3047815814614296\tGen. loss: 1.515380871295929\n",
      "Epoch 239\tDisc. loss: 0.31638005673885344\tGen. loss: 1.6943799376487731\n",
      "Epoch 240\tDisc. loss: 0.31894259750843046\tGen. loss: 1.5707138419151305\n",
      "Epoch 241\tDisc. loss: 0.30855591893196105\tGen. loss: 1.8862343549728393\n",
      "Epoch 242\tDisc. loss: 0.3134388029575348\tGen. loss: 1.635286033153534\n",
      "Epoch 243\tDisc. loss: 0.3170053392648697\tGen. loss: 1.628914201259613\n",
      "Epoch 244\tDisc. loss: 0.2981032535433769\tGen. loss: 1.8442535758018495\n",
      "Epoch 245\tDisc. loss: 0.29144154489040375\tGen. loss: 1.8961447715759276\n",
      "Epoch 246\tDisc. loss: 0.28581011295318604\tGen. loss: 1.9421762108802796\n",
      "Epoch 247\tDisc. loss: 0.283208429813385\tGen. loss: 1.9662811875343322\n",
      "Epoch 248\tDisc. loss: 0.31264127790927887\tGen. loss: 2.125841665267944\n",
      "Epoch 249\tDisc. loss: 0.32192196547985075\tGen. loss: 1.866879951953888\n",
      "Epoch 250\tDisc. loss: 0.3328858524560928\tGen. loss: 1.8299460768699647\n",
      "Epoch 251\tDisc. loss: 0.32357471287250517\tGen. loss: 1.8197544932365417\n",
      "Epoch 252\tDisc. loss: 0.30306770354509355\tGen. loss: 2.0616472363471985\n",
      "Epoch 253\tDisc. loss: 0.2922176793217659\tGen. loss: 1.7365375638008118\n",
      "Epoch 254\tDisc. loss: 0.2949302837252617\tGen. loss: 1.8028920769691468\n",
      "Epoch 255\tDisc. loss: 0.2904933035373688\tGen. loss: 1.7550730466842652\n",
      "Epoch 256\tDisc. loss: 0.3149006798863411\tGen. loss: 1.8668501019477843\n",
      "Epoch 257\tDisc. loss: 0.32202399522066116\tGen. loss: 1.6381975412368774\n",
      "Epoch 258\tDisc. loss: 0.3598276227712631\tGen. loss: 1.698986327648163\n",
      "Epoch 259\tDisc. loss: 0.30902252793312074\tGen. loss: 1.6947798371315002\n",
      "Epoch 260\tDisc. loss: 0.2929693639278412\tGen. loss: 1.7215607523918153\n",
      "Epoch 261\tDisc. loss: 0.2900301545858383\tGen. loss: 1.7020727276802063\n",
      "Epoch 262\tDisc. loss: 0.2825413703918457\tGen. loss: 1.8730263233184814\n",
      "Epoch 263\tDisc. loss: 0.29344851821660994\tGen. loss: 1.8848458528518677\n",
      "Epoch 264\tDisc. loss: 0.3132115676999092\tGen. loss: 1.8873055696487426\n",
      "Epoch 265\tDisc. loss: 0.31328869313001634\tGen. loss: 1.8349815607070923\n",
      "Epoch 266\tDisc. loss: 0.30225098580121995\tGen. loss: 1.745785391330719\n",
      "Epoch 267\tDisc. loss: 0.31025231778621676\tGen. loss: 1.766143226623535\n",
      "Epoch 268\tDisc. loss: 0.2914963364601135\tGen. loss: 1.9089457035064696\n",
      "Epoch 269\tDisc. loss: 0.2955435156822205\tGen. loss: 1.6312461972236634\n",
      "Epoch 270\tDisc. loss: 0.297872893512249\tGen. loss: 1.8495725750923158\n",
      "Epoch 271\tDisc. loss: 0.29777124971151353\tGen. loss: 1.8839777112007141\n",
      "Epoch 272\tDisc. loss: 0.29963477700948715\tGen. loss: 1.7709152579307557\n",
      "Epoch 273\tDisc. loss: 0.30403906404972075\tGen. loss: 2.0522717356681826\n",
      "Epoch 274\tDisc. loss: 0.29380999207496644\tGen. loss: 1.7282307863235473\n",
      "Epoch 275\tDisc. loss: 0.3068207412958145\tGen. loss: 2.2832995414733888\n",
      "Epoch 276\tDisc. loss: 0.28758445382118225\tGen. loss: 1.8432802200317382\n",
      "Epoch 277\tDisc. loss: 0.31351248174905777\tGen. loss: 2.178808319568634\n",
      "Epoch 278\tDisc. loss: 0.31869074553251264\tGen. loss: 1.750169587135315\n",
      "Epoch 279\tDisc. loss: 0.2964122876524925\tGen. loss: 1.8993203163146972\n",
      "Epoch 280\tDisc. loss: 0.30992987006902695\tGen. loss: 1.736211609840393\n",
      "Epoch 281\tDisc. loss: 0.2936056226491928\tGen. loss: 1.8379212498664856\n",
      "Epoch 282\tDisc. loss: 0.285294009745121\tGen. loss: 1.729503571987152\n",
      "Epoch 283\tDisc. loss: 0.2880324453115463\tGen. loss: 1.894067919254303\n",
      "Epoch 284\tDisc. loss: 0.30086175054311753\tGen. loss: 1.8293367981910706\n",
      "Epoch 285\tDisc. loss: 0.30964748114347457\tGen. loss: 1.7497269630432128\n",
      "Epoch 286\tDisc. loss: 0.30105754286050795\tGen. loss: 1.8725932598114015\n",
      "Epoch 287\tDisc. loss: 0.3102597281336784\tGen. loss: 1.8882590770721435\n",
      "Epoch 288\tDisc. loss: 0.30149922519922256\tGen. loss: 1.8786311626434327\n",
      "Epoch 289\tDisc. loss: 0.2878001436591148\tGen. loss: 1.9005897402763368\n",
      "Epoch 290\tDisc. loss: 0.27831943333148956\tGen. loss: 1.8251197695732118\n",
      "Epoch 291\tDisc. loss: 0.3004259243607521\tGen. loss: 1.9815546035766602\n",
      "Epoch 292\tDisc. loss: 0.28618112206459045\tGen. loss: 1.9236832141876221\n",
      "Epoch 293\tDisc. loss: 0.2891891196370125\tGen. loss: 1.7800350546836854\n",
      "Epoch 294\tDisc. loss: 0.3221874713897705\tGen. loss: 2.106480050086975\n",
      "Epoch 295\tDisc. loss: 0.30988783240318296\tGen. loss: 1.7284489035606385\n",
      "Epoch 296\tDisc. loss: 0.2958376303315163\tGen. loss: 1.7802009344100953\n",
      "Epoch 297\tDisc. loss: 0.29632620215415956\tGen. loss: 1.8276968836784362\n",
      "Epoch 298\tDisc. loss: 0.2898446410894394\tGen. loss: 1.615755546092987\n",
      "Epoch 299\tDisc. loss: 0.287650503218174\tGen. loss: 1.9470813751220704\n",
      "Epoch 300\tDisc. loss: 0.28821819424629214\tGen. loss: 1.8582080006599426\n",
      "Epoch 301\tDisc. loss: 0.2993470788002014\tGen. loss: 1.834269642829895\n",
      "Epoch 302\tDisc. loss: 0.30854701101779936\tGen. loss: 1.8444342613220215\n",
      "Epoch 303\tDisc. loss: 0.3005368486046791\tGen. loss: 1.8672317147254944\n",
      "Epoch 304\tDisc. loss: 0.289303657412529\tGen. loss: 1.8709749937057496\n",
      "Epoch 305\tDisc. loss: 0.2931350141763687\tGen. loss: 1.9161931037902833\n",
      "Epoch 306\tDisc. loss: 0.29006424099206923\tGen. loss: 1.9107749223709107\n",
      "Epoch 307\tDisc. loss: 0.29686849266290666\tGen. loss: 1.967919385433197\n",
      "Epoch 308\tDisc. loss: 0.3181803524494171\tGen. loss: 2.072408580780029\n",
      "Epoch 309\tDisc. loss: 0.29532805979251864\tGen. loss: 1.7564684629440308\n",
      "Epoch 310\tDisc. loss: 0.29125742316246034\tGen. loss: 1.9423888802528382\n",
      "Epoch 311\tDisc. loss: 0.28413925766944886\tGen. loss: 1.6568464279174804\n",
      "Epoch 312\tDisc. loss: 0.29565814435482024\tGen. loss: 1.7974485635757447\n",
      "Epoch 313\tDisc. loss: 0.28463653326034544\tGen. loss: 1.8089118599891663\n",
      "Epoch 314\tDisc. loss: 0.309342648088932\tGen. loss: 1.7718057751655578\n",
      "Epoch 315\tDisc. loss: 0.2982638329267502\tGen. loss: 1.5943337440490724\n",
      "Epoch 316\tDisc. loss: 0.28944154232740404\tGen. loss: 1.834990179538727\n",
      "Epoch 317\tDisc. loss: 0.29037133455276487\tGen. loss: 1.7383190274238587\n",
      "Epoch 318\tDisc. loss: 0.301065431535244\tGen. loss: 1.6714895606040954\n",
      "Epoch 319\tDisc. loss: 0.29444151669740676\tGen. loss: 2.0153287410736085\n",
      "Epoch 320\tDisc. loss: 0.29039818197488787\tGen. loss: 1.6389952301979065\n",
      "Epoch 321\tDisc. loss: 0.27677931934595107\tGen. loss: 1.9779001712799071\n",
      "Epoch 322\tDisc. loss: 0.2938639521598816\tGen. loss: 1.8500224828720093\n",
      "Epoch 323\tDisc. loss: 0.2830224514007568\tGen. loss: 1.97148939371109\n",
      "Epoch 324\tDisc. loss: 0.2966619715094566\tGen. loss: 1.947217547893524\n",
      "Epoch 325\tDisc. loss: 0.2867434173822403\tGen. loss: 1.7325268507003784\n",
      "Epoch 326\tDisc. loss: 0.28811332285404206\tGen. loss: 1.9618095874786377\n",
      "Epoch 327\tDisc. loss: 0.28464468419551847\tGen. loss: 2.065122890472412\n",
      "Epoch 328\tDisc. loss: 0.305618542432785\tGen. loss: 1.771546721458435\n",
      "Epoch 329\tDisc. loss: 0.29337573796510696\tGen. loss: 1.861975359916687\n",
      "Epoch 330\tDisc. loss: 0.2822997897863388\tGen. loss: 1.7549979209899902\n",
      "Epoch 331\tDisc. loss: 0.29355301707983017\tGen. loss: 1.865843367576599\n",
      "Epoch 332\tDisc. loss: 0.3108734980225563\tGen. loss: 1.8555221557617188\n",
      "Epoch 333\tDisc. loss: 0.294566310942173\tGen. loss: 1.5584111571311952\n",
      "Epoch 334\tDisc. loss: 0.2673316866159439\tGen. loss: 1.8646262764930726\n",
      "Epoch 335\tDisc. loss: 0.2764770194888115\tGen. loss: 1.7263131499290467\n",
      "Epoch 336\tDisc. loss: 0.277137853205204\tGen. loss: 1.7630563974380493\n",
      "Epoch 337\tDisc. loss: 0.2941757500171661\tGen. loss: 1.9185801863670349\n",
      "Epoch 338\tDisc. loss: 0.2843726396560669\tGen. loss: 1.7880187511444092\n",
      "Epoch 339\tDisc. loss: 0.2888829082250595\tGen. loss: 1.8791746735572814\n",
      "Epoch 340\tDisc. loss: 0.279678201675415\tGen. loss: 1.9567449808120727\n",
      "Epoch 341\tDisc. loss: 0.2882935479283333\tGen. loss: 1.9210869312286376\n",
      "Epoch 342\tDisc. loss: 0.28087610751390457\tGen. loss: 1.626003348827362\n",
      "Epoch 343\tDisc. loss: 0.29015744775533675\tGen. loss: 1.8186810612678528\n",
      "Epoch 344\tDisc. loss: 0.27484102472662925\tGen. loss: 1.6915313601493835\n",
      "Epoch 345\tDisc. loss: 0.2869293615221977\tGen. loss: 2.279656267166138\n",
      "Epoch 346\tDisc. loss: 0.30457210540771484\tGen. loss: 1.8215658068656921\n",
      "Epoch 347\tDisc. loss: 0.290975584089756\tGen. loss: 1.848670494556427\n",
      "Epoch 348\tDisc. loss: 0.3003539085388184\tGen. loss: 1.8676584362983704\n",
      "Epoch 349\tDisc. loss: 0.28681920170784\tGen. loss: 1.7401620030403138\n",
      "Epoch 350\tDisc. loss: 0.3116848796606064\tGen. loss: 1.9334455966949462\n",
      "Epoch 351\tDisc. loss: 0.29356697499752044\tGen. loss: 1.6590065360069275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 352\tDisc. loss: 0.31066514253616334\tGen. loss: 2.1222018361091615\n",
      "Epoch 353\tDisc. loss: 0.29252957701683047\tGen. loss: 1.752414071559906\n",
      "Epoch 354\tDisc. loss: 0.2820185899734497\tGen. loss: 1.9546061992645263\n",
      "Epoch 355\tDisc. loss: 0.3075859323143959\tGen. loss: 1.9324840307235718\n",
      "Epoch 356\tDisc. loss: 0.2920322477817535\tGen. loss: 1.7749521613121033\n",
      "Epoch 357\tDisc. loss: 0.29260424226522447\tGen. loss: 1.9022231578826905\n",
      "Epoch 358\tDisc. loss: 0.2985881924629211\tGen. loss: 1.7934437274932862\n",
      "Epoch 359\tDisc. loss: 0.3033438980579376\tGen. loss: 1.7288247704505921\n",
      "Epoch 360\tDisc. loss: 0.2874980464577675\tGen. loss: 1.8387707948684693\n",
      "Epoch 361\tDisc. loss: 0.2874173760414124\tGen. loss: 1.8188298702239991\n",
      "Epoch 362\tDisc. loss: 0.2881524160504341\tGen. loss: 1.7882270097732544\n",
      "Epoch 363\tDisc. loss: 0.28658811151981356\tGen. loss: 1.9082687854766847\n",
      "Epoch 364\tDisc. loss: 0.2792623907327652\tGen. loss: 1.8794975876808167\n",
      "Epoch 365\tDisc. loss: 0.2867651268839836\tGen. loss: 2.086746597290039\n",
      "Epoch 366\tDisc. loss: 0.2891874000430107\tGen. loss: 1.9568012237548829\n",
      "Epoch 367\tDisc. loss: 0.28899892419576645\tGen. loss: 1.7431782245635987\n",
      "Epoch 368\tDisc. loss: 0.2793677613139153\tGen. loss: 1.999263346195221\n",
      "Epoch 369\tDisc. loss: 0.2976026996970177\tGen. loss: 2.265399527549744\n",
      "Epoch 370\tDisc. loss: 0.2835831806063652\tGen. loss: 1.7023075222969055\n",
      "Epoch 371\tDisc. loss: 0.3004472255706787\tGen. loss: 2.0095167756080627\n",
      "Epoch 372\tDisc. loss: 0.27907279431819915\tGen. loss: 1.8532497882843018\n",
      "Epoch 373\tDisc. loss: 0.30305332541465757\tGen. loss: 1.7609176158905029\n",
      "Epoch 374\tDisc. loss: 0.2859088271856308\tGen. loss: 1.713235580921173\n",
      "Epoch 375\tDisc. loss: 0.29678146094083785\tGen. loss: 1.9902966022491455\n",
      "Epoch 376\tDisc. loss: 0.29384912699460985\tGen. loss: 1.982663905620575\n",
      "Epoch 377\tDisc. loss: 0.29275178760290144\tGen. loss: 2.0892730712890626\n",
      "Epoch 378\tDisc. loss: 0.29426404684782026\tGen. loss: 2.0841856002807617\n",
      "Epoch 379\tDisc. loss: 0.2802623510360718\tGen. loss: 2.203260827064514\n",
      "Epoch 380\tDisc. loss: 0.2883032411336899\tGen. loss: 1.8818454146385193\n",
      "Epoch 381\tDisc. loss: 0.29249664694070815\tGen. loss: 1.775484025478363\n",
      "Epoch 382\tDisc. loss: 0.2863100603222847\tGen. loss: 1.6737713932991027\n",
      "Epoch 383\tDisc. loss: 0.27571144700050354\tGen. loss: 2.129320204257965\n",
      "Epoch 384\tDisc. loss: 0.2791722521185875\tGen. loss: 1.8631643414497376\n",
      "Epoch 385\tDisc. loss: 0.27945745438337327\tGen. loss: 1.8545847296714784\n",
      "Epoch 386\tDisc. loss: 0.2823932856321335\tGen. loss: 2.018292021751404\n",
      "Epoch 387\tDisc. loss: 0.2991180807352066\tGen. loss: 1.804533314704895\n",
      "Epoch 388\tDisc. loss: 0.3026577889919281\tGen. loss: 1.7694367051124573\n",
      "Epoch 389\tDisc. loss: 0.29032176584005354\tGen. loss: 1.754460370540619\n",
      "Epoch 390\tDisc. loss: 0.2780337750911713\tGen. loss: 1.8884304523468018\n",
      "Epoch 391\tDisc. loss: 0.2914584934711456\tGen. loss: 1.907144832611084\n",
      "Epoch 392\tDisc. loss: 0.3043833404779434\tGen. loss: 1.8049827098846436\n",
      "Epoch 393\tDisc. loss: 0.2954446583986282\tGen. loss: 2.2465838193893433\n",
      "Epoch 394\tDisc. loss: 0.28982036709785464\tGen. loss: 1.9620704293251037\n",
      "Epoch 395\tDisc. loss: 0.30806236863136294\tGen. loss: 2.004579257965088\n",
      "Epoch 396\tDisc. loss: 0.2824786067008972\tGen. loss: 1.9325805306434631\n",
      "Epoch 397\tDisc. loss: 0.2964961975812912\tGen. loss: 2.1186975479125976\n",
      "Epoch 398\tDisc. loss: 0.29433396458625793\tGen. loss: 2.1673911094665526\n",
      "Epoch 399\tDisc. loss: 0.29263084232807157\tGen. loss: 1.6435539484024049\n",
      "Epoch 400\tDisc. loss: 0.28635168820619583\tGen. loss: 1.7999135851860046\n",
      "Epoch 401\tDisc. loss: 0.28717959076166155\tGen. loss: 1.8845368862152099\n",
      "Epoch 402\tDisc. loss: 0.2836934491991997\tGen. loss: 1.8741701602935792\n",
      "Epoch 403\tDisc. loss: 0.285426552593708\tGen. loss: 1.8735135197639465\n",
      "Epoch 404\tDisc. loss: 0.2881780579686165\tGen. loss: 2.26638126373291\n",
      "Epoch 405\tDisc. loss: 0.2874628692865372\tGen. loss: 1.8620031356811524\n",
      "Epoch 406\tDisc. loss: 0.273020350933075\tGen. loss: 1.849037504196167\n",
      "Epoch 407\tDisc. loss: 0.26835369765758516\tGen. loss: 1.972492778301239\n",
      "Epoch 408\tDisc. loss: 0.272056782245636\tGen. loss: 1.9071239829063416\n",
      "Epoch 409\tDisc. loss: 0.2787507802248001\tGen. loss: 2.3963112831115723\n",
      "Epoch 410\tDisc. loss: 0.29312868118286134\tGen. loss: 2.0438375949859617\n",
      "Epoch 411\tDisc. loss: 0.279468309879303\tGen. loss: 2.1132054567337035\n",
      "Epoch 412\tDisc. loss: 0.28755992501974104\tGen. loss: 1.9225159645080567\n",
      "Epoch 413\tDisc. loss: 0.28635244965553286\tGen. loss: 1.915694236755371\n",
      "Epoch 414\tDisc. loss: 0.27992952466011045\tGen. loss: 1.6993436932563781\n",
      "Epoch 415\tDisc. loss: 0.27181429862976075\tGen. loss: 1.643308663368225\n",
      "Epoch 416\tDisc. loss: 0.3073669895529747\tGen. loss: 1.9770666003227233\n",
      "Epoch 417\tDisc. loss: 0.2823124334216118\tGen. loss: 1.6143909454345704\n",
      "Epoch 418\tDisc. loss: 0.2778156533837318\tGen. loss: 2.047146534919739\n",
      "Epoch 419\tDisc. loss: 0.2692703872919083\tGen. loss: 1.7053327083587646\n",
      "Epoch 420\tDisc. loss: 0.2640235722064972\tGen. loss: 1.9359360933303833\n",
      "Epoch 421\tDisc. loss: 0.26864044815301896\tGen. loss: 2.1726873397827147\n",
      "Epoch 422\tDisc. loss: 0.27401103228330614\tGen. loss: 1.99618239402771\n",
      "Epoch 423\tDisc. loss: 0.2747096136212349\tGen. loss: 1.8233321547508239\n",
      "Epoch 424\tDisc. loss: 0.2784224063158035\tGen. loss: 2.0108450412750245\n",
      "Epoch 425\tDisc. loss: 0.28281952142715455\tGen. loss: 1.940607213973999\n",
      "Epoch 426\tDisc. loss: 0.2835003525018692\tGen. loss: 1.844573163986206\n",
      "Epoch 427\tDisc. loss: 0.29940624386072157\tGen. loss: 2.1278461098670958\n",
      "Epoch 428\tDisc. loss: 0.26247574388980865\tGen. loss: 1.92433021068573\n",
      "Epoch 429\tDisc. loss: 0.268587264418602\tGen. loss: 2.340702843666077\n",
      "Epoch 430\tDisc. loss: 0.2854383334517479\tGen. loss: 2.1318339109420776\n",
      "Epoch 431\tDisc. loss: 0.27572747468948366\tGen. loss: 2.1046991109848023\n",
      "Epoch 432\tDisc. loss: 0.27250629365444184\tGen. loss: 2.1124542713165284\n",
      "Epoch 433\tDisc. loss: 0.2717043191194534\tGen. loss: 1.5891645193099975\n",
      "Epoch 434\tDisc. loss: 0.2740422859787941\tGen. loss: 2.0642114996910097\n",
      "Epoch 435\tDisc. loss: 0.28981304317712786\tGen. loss: 1.9815545797348022\n",
      "Epoch 436\tDisc. loss: 0.2720202997326851\tGen. loss: 1.7299901247024536\n",
      "Epoch 437\tDisc. loss: 0.26802426427602766\tGen. loss: 2.072598910331726\n",
      "Epoch 438\tDisc. loss: 0.29075086265802386\tGen. loss: 1.8739344835281373\n",
      "Epoch 439\tDisc. loss: 0.2873131990432739\tGen. loss: 1.9711268186569213\n",
      "Epoch 440\tDisc. loss: 0.27433202415704727\tGen. loss: 1.8687638521194458\n",
      "Epoch 441\tDisc. loss: 0.27221898436546327\tGen. loss: 2.0016946673393248\n",
      "Epoch 442\tDisc. loss: 0.26244325935840607\tGen. loss: 1.83909010887146\n",
      "Epoch 443\tDisc. loss: 0.2899586379528046\tGen. loss: 2.0595663189888\n",
      "Epoch 444\tDisc. loss: 0.29556758999824523\tGen. loss: 1.7760046601295472\n",
      "Epoch 445\tDisc. loss: 0.286926431953907\tGen. loss: 2.3451555490493776\n",
      "Epoch 446\tDisc. loss: 0.2794804275035858\tGen. loss: 1.7611434817314149\n",
      "Epoch 447\tDisc. loss: 0.2663865551352501\tGen. loss: 1.8840767741203308\n",
      "Epoch 448\tDisc. loss: 0.2821792423725128\tGen. loss: 1.8263688921928405\n",
      "Epoch 449\tDisc. loss: 0.3157647177577019\tGen. loss: 2.0618942022323608\n",
      "Epoch 450\tDisc. loss: 0.27411154061555865\tGen. loss: 1.7082826018333435\n",
      "Epoch 451\tDisc. loss: 0.27038447707891466\tGen. loss: 1.9334097743034362\n",
      "Epoch 452\tDisc. loss: 0.26894426345825195\tGen. loss: 2.1389037132263184\n",
      "Epoch 453\tDisc. loss: 0.27391625940799713\tGen. loss: 1.7995248198509217\n",
      "Epoch 454\tDisc. loss: 0.2609351217746735\tGen. loss: 2.086659860610962\n",
      "Epoch 455\tDisc. loss: 0.2689765438437462\tGen. loss: 1.9993242383003236\n",
      "Epoch 456\tDisc. loss: 0.28365638107061386\tGen. loss: 1.8682288527488708\n",
      "Epoch 457\tDisc. loss: 0.27175638526678086\tGen. loss: 1.9930371284484862\n",
      "Epoch 458\tDisc. loss: 0.2632847517728806\tGen. loss: 1.9740622162818908\n",
      "Epoch 459\tDisc. loss: 0.281048484146595\tGen. loss: 2.0885712623596193\n",
      "Epoch 460\tDisc. loss: 0.2668688416481018\tGen. loss: 1.9970264077186584\n",
      "Epoch 461\tDisc. loss: 0.27231552451848984\tGen. loss: 1.8792012810707093\n",
      "Epoch 462\tDisc. loss: 0.28667051047086717\tGen. loss: 2.1553766250610353\n",
      "Epoch 463\tDisc. loss: 0.26002638339996337\tGen. loss: 1.8890473127365113\n",
      "Epoch 464\tDisc. loss: 0.2849604547023773\tGen. loss: 1.838964009284973\n",
      "Epoch 465\tDisc. loss: 0.27078352868556976\tGen. loss: 1.7553691864013672\n",
      "Epoch 466\tDisc. loss: 0.2612201049923897\tGen. loss: 1.81800217628479\n",
      "Epoch 467\tDisc. loss: 0.2722060173749924\tGen. loss: 1.963154149055481\n",
      "Epoch 468\tDisc. loss: 0.2602533310651779\tGen. loss: 2.0086023926734926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 469\tDisc. loss: 0.2588779076933861\tGen. loss: 2.11241672039032\n",
      "Epoch 470\tDisc. loss: 0.25131491124629973\tGen. loss: 2.0913439750671388\n",
      "Epoch 471\tDisc. loss: 0.25567798018455506\tGen. loss: 2.055191922187805\n",
      "Epoch 472\tDisc. loss: 0.28386152982711793\tGen. loss: 2.057574462890625\n",
      "Epoch 473\tDisc. loss: 0.26811091154813765\tGen. loss: 1.786786103248596\n",
      "Epoch 474\tDisc. loss: 0.26578719168901443\tGen. loss: 2.0264625191688537\n",
      "Epoch 475\tDisc. loss: 0.2588771864771843\tGen. loss: 1.8848705410957336\n",
      "Epoch 476\tDisc. loss: 0.26912054866552354\tGen. loss: 2.1307661533355713\n",
      "Epoch 477\tDisc. loss: 0.26859532445669176\tGen. loss: 1.889698040485382\n",
      "Epoch 478\tDisc. loss: 0.2701322391629219\tGen. loss: 2.099729299545288\n",
      "Epoch 479\tDisc. loss: 0.263787205517292\tGen. loss: 1.8567912340164185\n",
      "Epoch 480\tDisc. loss: 0.27165959030389786\tGen. loss: 1.7822728276252746\n",
      "Epoch 481\tDisc. loss: 0.254658454656601\tGen. loss: 1.8584864377975463\n",
      "Epoch 482\tDisc. loss: 0.2693925142288208\tGen. loss: 2.0648998737335207\n",
      "Epoch 483\tDisc. loss: 0.26414577662944794\tGen. loss: 1.9141048789024353\n",
      "Epoch 484\tDisc. loss: 0.2664118275046349\tGen. loss: 2.189471650123596\n",
      "Epoch 485\tDisc. loss: 0.27555058151483536\tGen. loss: 2.1193220138549806\n",
      "Epoch 486\tDisc. loss: 0.25247189253568647\tGen. loss: 1.946490752696991\n",
      "Epoch 487\tDisc. loss: 0.26545572429895403\tGen. loss: 1.8437790274620056\n",
      "Epoch 488\tDisc. loss: 0.26216573268175125\tGen. loss: 2.024626588821411\n",
      "Epoch 489\tDisc. loss: 0.2831655219197273\tGen. loss: 2.1629145860672\n",
      "Epoch 490\tDisc. loss: 0.26237029284238816\tGen. loss: 2.151973009109497\n",
      "Epoch 491\tDisc. loss: 0.2684450387954712\tGen. loss: 1.86372252702713\n",
      "Epoch 492\tDisc. loss: 0.26686997562646864\tGen. loss: 1.8093978762626648\n",
      "Epoch 493\tDisc. loss: 0.26797088235616684\tGen. loss: 2.0040548205375672\n",
      "Epoch 494\tDisc. loss: 0.26880761682987214\tGen. loss: 1.9747260451316833\n",
      "Epoch 495\tDisc. loss: 0.2847012013196945\tGen. loss: 1.8072888731956482\n",
      "Epoch 496\tDisc. loss: 0.2607292801141739\tGen. loss: 2.029156517982483\n",
      "Epoch 497\tDisc. loss: 0.265011802315712\tGen. loss: 2.017030382156372\n",
      "Epoch 498\tDisc. loss: 0.27611819803714754\tGen. loss: 2.1485570669174194\n",
      "Epoch 499\tDisc. loss: 0.273795573413372\tGen. loss: 1.9943489909172059\n",
      "Epoch 500\tDisc. loss: 0.2551318034529686\tGen. loss: 2.2098409652709963\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    print(f'Epoch {i+1}\\t', end='')\n",
    "    disc_losses = []\n",
    "    gen_losses = []\n",
    "    for true_data in X_train_loader:\n",
    "        #### DISCRIMINATOR OPTMIZATION ####\n",
    "        n_data = true_data.shape[0]\n",
    "        \n",
    "        discriminator.train() # Unfreeze parameters of the discriminator\n",
    "        generator.eval() # Freeze parameters of the generator\n",
    "        \n",
    "        disc_optimizer.zero_grad()\n",
    "        \n",
    "        true_x = true_data.to(device) # true data from the training dataset\n",
    "        noise = torch.randn(n_data, noise_dim).to(device)\n",
    "        fake_x = generator(noise).detach() # fake data from the noise distribution ~ N(0, 1)\n",
    "\n",
    "        x = torch.cat([true_x, fake_x]) # Gather true and fake data\n",
    "        \n",
    "        \n",
    "        true_y = torch.ones((n_data,), dtype=torch.long).to(device) # target 1 for true data\n",
    "        fake_y = torch.zeros((n_data,), dtype=torch.long).to(device) # target 0 for fake data\n",
    "        \n",
    "        y = torch.cat([true_y, fake_y]) # Gather true and fake targets\n",
    "        \n",
    "        \n",
    "        \n",
    "        output = discriminator(x)\n",
    "        \n",
    "        disc_loss = criterion(output, y) # Penalize Discriminator for failing to distinguish fake data from true data\n",
    "        disc_losses.append(disc_loss.item())\n",
    "        \n",
    "        disc_loss.backward()\n",
    "        \n",
    "        \n",
    "        disc_optimizer.step() # Optimize Discriminator\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### GENERATOR OPTIMIZATION ###\n",
    "        n_data = true_data.shape[0] * 4\n",
    "    \n",
    "        discriminator.eval() # Freeze parameters of the discriminator\n",
    "        generator.train() # Unfreeze parameters of the generator\n",
    "\n",
    "        gen_optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        noise = torch.randn(n_data, noise_dim).to(device)\n",
    "        fake_x = generator(noise) # fake data from the noise distribution ~ N(0, 1)\n",
    "        \n",
    "        true_y = torch.ones((n_data,), dtype=torch.long).to(device) # target 1 for true data\n",
    "        \n",
    "        output = discriminator(fake_x)\n",
    "        \n",
    "        gen_loss = criterion(output, true_y) # Penalize Generator for failing to fool the discriminator\n",
    "        gen_losses.append(gen_loss.item())\n",
    "        \n",
    "        gen_loss.backward()\n",
    "        \n",
    "\n",
    "        gen_optimizer.step() # Optimize Generator  \n",
    "    \n",
    "    \n",
    "    disc_loss = np.mean(disc_losses)\n",
    "    gen_loss = np.mean(gen_losses)\n",
    "    \n",
    "    print(f'Disc. loss: {disc_loss}\\t', end='')\n",
    "    print(f'Gen. loss: {gen_loss}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=256, bias=True)\n",
       "  (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): LeakyReLU(negative_slope=0.01)\n",
       "  (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): LeakyReLU(negative_slope=0.01)\n",
       "  (6): Linear(in_features=64, out_features=4, bias=True)\n",
       "  (7): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator.eval()\n",
    "generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator(generator(torch.randn(batch_size, noise_dim).to(device))).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator(X_eval.to(device)).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0131, 0.0128, 0.0094, 0.0108],\n",
       "        [0.0242, 0.0307, 0.0338, 0.0211],\n",
       "        [0.0317, 0.0236, 0.0154, 0.0108],\n",
       "        [0.0228, 0.0258, 0.0274, 0.0241],\n",
       "        [0.0343, 0.0219, 0.0172, 0.0149],\n",
       "        [0.0427, 0.0604, 0.0329, 0.0245],\n",
       "        [0.0131, 0.0154, 0.0094, 0.0108],\n",
       "        [0.0131, 0.0128, 0.0094, 0.0108],\n",
       "        [0.0248, 0.0265, 0.0094, 0.0188],\n",
       "        [0.0131, 0.0128, 0.0094, 0.0108],\n",
       "        [0.0258, 0.0277, 0.0094, 0.0108],\n",
       "        [0.0763, 0.0258, 0.0153, 0.0223],\n",
       "        [0.0227, 0.0128, 0.0094, 0.0195],\n",
       "        [0.0131, 0.0128, 0.0163, 0.0108],\n",
       "        [0.0356, 0.0383, 0.0370, 0.0344],\n",
       "        [0.0131, 0.0213, 0.0094, 0.0108],\n",
       "        [0.0779, 0.0568, 0.0366, 0.0509],\n",
       "        [0.0238, 0.0298, 0.0332, 0.0248],\n",
       "        [0.0197, 0.0212, 0.0192, 0.0182],\n",
       "        [0.0522, 0.0501, 0.0165, 0.0108],\n",
       "        [0.0235, 0.0128, 0.0094, 0.0217],\n",
       "        [0.0131, 0.0128, 0.0094, 0.0108],\n",
       "        [0.0131, 0.0128, 0.0180, 0.0162],\n",
       "        [0.0236, 0.0298, 0.0094, 0.0227],\n",
       "        [0.0362, 0.0257, 0.0164, 0.0108],\n",
       "        [0.0412, 0.0347, 0.0362, 0.0423],\n",
       "        [0.0348, 0.0341, 0.0369, 0.0367],\n",
       "        [0.0461, 0.0244, 0.0172, 0.0187],\n",
       "        [0.0330, 0.0262, 0.0094, 0.0134],\n",
       "        [0.0312, 0.0283, 0.0094, 0.0108],\n",
       "        [0.0131, 0.0128, 0.0146, 0.0108],\n",
       "        [0.0131, 0.0128, 0.0094, 0.0108],\n",
       "        [0.0248, 0.0128, 0.0179, 0.0198],\n",
       "        [0.0612, 0.0244, 0.0156, 0.0189],\n",
       "        [0.0259, 0.0257, 0.0094, 0.0108],\n",
       "        [0.0237, 0.0250, 0.0094, 0.0193],\n",
       "        [0.0241, 0.0266, 0.0263, 0.0257],\n",
       "        [0.0652, 0.0236, 0.0136, 0.0163],\n",
       "        [0.0259, 0.0265, 0.0246, 0.0201],\n",
       "        [0.0131, 0.0209, 0.0198, 0.0108],\n",
       "        [0.0722, 0.0240, 0.0145, 0.0195],\n",
       "        [0.0131, 0.0128, 0.0094, 0.0108],\n",
       "        [0.0193, 0.0222, 0.0094, 0.0108],\n",
       "        [0.0202, 0.0215, 0.0199, 0.0188],\n",
       "        [0.0291, 0.0128, 0.0178, 0.0184],\n",
       "        [0.0131, 0.0128, 0.0094, 0.0108],\n",
       "        [0.0238, 0.0223, 0.0207, 0.0196],\n",
       "        [0.0301, 0.0279, 0.0094, 0.0108],\n",
       "        [0.0352, 0.0241, 0.0161, 0.0158],\n",
       "        [0.0390, 0.0396, 0.0193, 0.0113],\n",
       "        [0.0230, 0.0128, 0.0094, 0.0212],\n",
       "        [0.0284, 0.0234, 0.0189, 0.0181],\n",
       "        [0.0362, 0.0259, 0.0173, 0.0108],\n",
       "        [0.0274, 0.0128, 0.0094, 0.0108],\n",
       "        [0.0131, 0.0347, 0.0189, 0.0186],\n",
       "        [0.0243, 0.0311, 0.0290, 0.0227],\n",
       "        [0.0131, 0.0128, 0.0130, 0.0108],\n",
       "        [0.0131, 0.0128, 0.0094, 0.0108],\n",
       "        [0.0247, 0.0224, 0.0181, 0.0124],\n",
       "        [0.0131, 0.0203, 0.0203, 0.0108],\n",
       "        [0.0228, 0.0303, 0.0356, 0.0201],\n",
       "        [0.0188, 0.0270, 0.0226, 0.0188],\n",
       "        [0.0247, 0.0128, 0.0167, 0.0202],\n",
       "        [0.0239, 0.0128, 0.0094, 0.0207]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denorm(generator(torch.randn(batch_size, noise_dim).to(device)), mu.to(device), theta.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
